{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import scipy\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Not a class, just a bunch of useful functions.\n",
    "\n",
    "def get_chapter(module_path):\n",
    "    return re.search('_ch._', 'crowdsignals_ch3_outliers.py').group(0).strip('_')\n",
    "\n",
    "def normalize_dataset(data_table, columns):\n",
    "    dt_norm = copy.deepcopy(data_table)\n",
    "    for col in columns:\n",
    "        dt_norm[col] = (data_table[col] - data_table[col].mean()) / (data_table[col].max() - data_table[col].min())\n",
    "    return dt_norm\n",
    "\n",
    "# Calculate the distance between rows.\n",
    "def distance(rows, d_function='euclidean'):\n",
    "    if d_function == 'euclidean':\n",
    "        # Assumes m rows and n columns (attributes), returns and array where each row represents\n",
    "        # the distances to the other rows (except the own row).\n",
    "        return scipy.spatial.distance.pdist(rows, 'euclidean') # todo: replace with numpy?\n",
    "    else:\n",
    "        raise ValueError(\"Unknown distance value '\" + d_function + \"'\")\n",
    "\n",
    "def print_statistics(dataset, describe=True):\n",
    "\n",
    "    if describe:\n",
    "        # .describe() gives number of values, mean, standard deviation, min and max for each column in one table.\n",
    "        print(dataset.describe().round(3).to_string())\n",
    "        return\n",
    "\n",
    "    print('\\ncolumn \\t\\t % missing \\t\\t mean \\t\\t standard deviation \\t\\t min \\t\\t max')\n",
    "    dataset_length = len(dataset.index)\n",
    "    for col in dataset.columns:\n",
    "        print('\\t\\t'.join([f'{col}',\n",
    "                           f'{(dataset_length - dataset[col].count()) / dataset_length * 100:3.1f}%',\n",
    "                           f'{dataset[col].mean():6.3f}',\n",
    "                           f'{dataset[col].std():6.3f}',\n",
    "                           f'{dataset[col].min():6.3f}',\n",
    "                           f'{dataset[col].max():6.3f}']))\n",
    "\n",
    "def print_table_cell(value1, value2):\n",
    "    print(\"{0:.2f}\".format(value1), ' / ', \"{0:.2f}\".format(value2), end='')\n",
    "\n",
    "def print_latex_table_statistics_two_datasets(dataset1, dataset2):\n",
    "    print('attribute, fraction missing values, mean, standard deviation, min, max')\n",
    "    dataset1_length = len(dataset1.index)\n",
    "    dataset2_length = len(dataset2.index)\n",
    "    for col in dataset1.columns:\n",
    "        print(col, '& ', end='')\n",
    "        print_table_cell((float((dataset1_length - dataset1[col].count()))/dataset1_length)*100, (float((dataset2_length - dataset2[col].count()))/dataset2_length)*100)\n",
    "        print(' & ', end='')\n",
    "        print_table_cell(dataset1[col].mean(), dataset2[col].mean())\n",
    "        print(' & ', end='')\n",
    "        print_table_cell(dataset1[col].std(), dataset2[col].std())\n",
    "        print(' & ', end='')\n",
    "        print_table_cell(dataset1[col].min(), dataset2[col].min())\n",
    "        print(' & ', end='')\n",
    "        print_table_cell(dataset1[col].max(), dataset2[col].max())\n",
    "        print('\\\\\\\\')\n",
    "\n",
    "def print_latex_statistics_clusters(dataset, cluster_col, input_cols, label_col):\n",
    "    label_cols = [c for c in dataset.columns if label_col == c[0:len(label_col)]]\n",
    "\n",
    "    clusters = dataset[cluster_col].unique()\n",
    "\n",
    "    for c in input_cols:\n",
    "        print('\\multirow{2}{*}{', c, '} & mean ', end='')\n",
    "        for cluster in clusters:\n",
    "            print(' & ', \"{0:.2f}\".format(dataset.loc[dataset[cluster_col] == cluster, c].mean()), end='')\n",
    "        print('\\\\\\\\')\n",
    "        print(' & std ', end='')\n",
    "        for cluster in clusters:\n",
    "            print(' & ', \"{0:.2f}\".format(dataset.loc[dataset[cluster_col] == cluster, c].std()), end='')\n",
    "        print('\\\\\\\\')\n",
    "\n",
    "    for l in label_cols:\n",
    "        print(l, ' & percentage ', end='')\n",
    "        for cluster in clusters:\n",
    "            print(' & ', \"{0:.2f}\".format((float(dataset.loc[dataset[cluster_col] == cluster, l].sum())/len(dataset[dataset[l] == 1].index) * 100)), '\\%', end='')\n",
    "        print('\\\\\\\\')\n",
    "\n",
    "def print_table_row_performances(row_name, training_len, test_len, values):\n",
    "    scores_over_sd = []\n",
    "    print(row_name, end='')\n",
    "\n",
    "    for val in values:\n",
    "        print(' & ', end='')\n",
    "        sd_train = math.sqrt((val[0]*(1-val[0]))/training_len)\n",
    "        print(\"{0:.4f}\".format(val[0]), end='')\n",
    "        print('\\\\emph{(', \"{0:.4f}\".format(val[0]-2*sd_train), '-', \"{0:.4f}\".format(val[0]+2*sd_train), ')}', ' & ', end='')\n",
    "        sd_test = math.sqrt((val[1]*(1-val[1]))/test_len)\n",
    "        print(\"{0:.4f}\".format(val[1]), end='')\n",
    "        print('\\\\emph{(', \"{0:.4f}\".format(val[1]-2*sd_test), '-', \"{0:.4f}\".format(val[1]+2*sd_test), ')}', end='')\n",
    "        scores_over_sd.append([val[0], sd_train, val[1], sd_test])\n",
    "    print('\\\\\\\\\\\\hline')\n",
    "    return scores_over_sd\n",
    "\n",
    "def print_table_row_performances_regression(row_name, training_len, test_len, values):\n",
    "    print(row_name),\n",
    "\n",
    "    for val in values:\n",
    "        print(' & ', end='')\n",
    "        print(\"{0:.4f}\".format(val[0]), end='')\n",
    "        print('\\\\emph{(', \"{0:.4f}\".format(val[1]), ')}', ' & ', end='')\n",
    "        print(\"{0:.4f}\".format(val[2]), end='')\n",
    "        print('\\\\emph{(', \"{0:.4f}\".format(val[3]), ')}', end='')\n",
    "    print('\\\\\\\\\\\\hline')\n",
    "\n",
    "def print_pearson_correlations(correlations):\n",
    "    for i in range(0, len(correlations)):\n",
    "        if np.isfinite(correlations[i][1]):\n",
    "            print(correlations[i][0], ' & ', \"{0:.4f}\".format(correlations[i][1]), '\\\\\\\\\\\\hline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.dates as md\n",
    "\n",
    "\n",
    "class CreateDataset:\n",
    "\n",
    "    base_dir = ''\n",
    "    granularity = 0\n",
    "    data_table = None\n",
    "\n",
    "    def __init__(self, base_dir, granularity):\n",
    "        self.base_dir = base_dir\n",
    "        self.granularity = granularity\n",
    "\n",
    "    # Create an initial data table with entries from start till end time, with steps\n",
    "    # of size granularity. Granularity is specified in milliseconds\n",
    "    def create_timestamps(self, start_time, end_time):\n",
    "        return pd.date_range(start_time, end_time, freq=str(self.granularity)+'ms')\n",
    "\n",
    "    def create_dataset(self, start_time, end_time, cols, prefix):\n",
    "        c = copy.deepcopy(cols)\n",
    "        if not prefix == '':\n",
    "            for i in range(0, len(c)):\n",
    "                c[i] = str(prefix) + str(c[i])\n",
    "        timestamps = self.create_timestamps(start_time, end_time)\n",
    "        self.data_table = pd.DataFrame(index=timestamps, columns=c)\n",
    "\n",
    "    # Add numerical data, we assume timestamps in the form of nanoseconds from the epoch\n",
    "    def add_numerical_dataset(self, file, timestamp_col, value_cols, aggregation='avg', prefix=''):\n",
    "        print(f'Reading data from {file}')\n",
    "        dataset = pd.read_csv(self.base_dir / file, skipinitialspace=True)\n",
    "\n",
    "        # Convert timestamps to dates\n",
    "        dataset[timestamp_col] = pd.to_datetime(dataset[timestamp_col])\n",
    "\n",
    "        # Create a table based on the times found in the dataset\n",
    "        if self.data_table is None:\n",
    "            self.create_dataset(min(dataset[timestamp_col]), max(dataset[timestamp_col]), value_cols, prefix)\n",
    "        else:\n",
    "            for col in value_cols:\n",
    "                self.data_table[str(prefix) + str(col)] = np.nan\n",
    "\n",
    "        # Over all rows in the new table\n",
    "        for i in range(0, len(self.data_table.index)):\n",
    "            # Select the relevant measurements.\n",
    "            relevant_rows = dataset[\n",
    "                (dataset[timestamp_col] >= self.data_table.index[i]) &\n",
    "                (dataset[timestamp_col] < (self.data_table.index[i] +\n",
    "                                           timedelta(milliseconds=self.granularity)))\n",
    "            ]\n",
    "            for col in value_cols:\n",
    "                # Take the average value\n",
    "                if len(relevant_rows) > 0:\n",
    "                    if aggregation == 'avg':\n",
    "                        self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.average(relevant_rows[col])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown aggregation {aggregation}\")\n",
    "                else:\n",
    "                    self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.nan\n",
    "\n",
    "    # Remove undesired value from the names.\n",
    "    def clean_name(self, name):\n",
    "        return re.sub('[^0-9a-zA-Z]+', '', name)\n",
    "\n",
    "    # Add data in which we have rows that indicate the occurrence of a certain event with a given start and end time.\n",
    "    # 'aggregation' can be 'sum' or 'binary'.\n",
    "    def add_event_dataset(self, file, start_timestamp_col, end_timestamp_col, value_col, aggregation='sum'):\n",
    "        print(f'Reading data from {file}')\n",
    "        dataset = pd.read_csv(self.base_dir / file)\n",
    "\n",
    "        # Convert timestamps to datetime.\n",
    "        dataset[start_timestamp_col] = pd.to_datetime(dataset[start_timestamp_col])\n",
    "        dataset[end_timestamp_col] = pd.to_datetime(dataset[end_timestamp_col])\n",
    "\n",
    "        # Clean the event values in the dataset\n",
    "        dataset[value_col] = dataset[value_col].apply(self.clean_name)\n",
    "        event_values = dataset[value_col].unique()\n",
    "\n",
    "        # Add columns for all possible values (or create a new dataset if empty), set the default to 0 occurrences\n",
    "        if self.data_table is None:\n",
    "            self.create_dataset(min(dataset[start_timestamp_col]), max(dataset[end_timestamp_col]), event_values, value_col)\n",
    "        for col in event_values:\n",
    "            self.data_table[(str(value_col) + str(col))] = 0\n",
    "\n",
    "        # Now we need to start counting by passing along the rows....\n",
    "        for i in range(0, len(dataset.index)):\n",
    "            # identify the time points of the row in our dataset and the value\n",
    "            start = dataset[start_timestamp_col][i]\n",
    "            end = dataset[end_timestamp_col][i]\n",
    "            value = dataset[value_col][i]\n",
    "            border = (start - timedelta(milliseconds=self.granularity))\n",
    "\n",
    "            # get the right rows from our data table\n",
    "            relevant_rows = self.data_table[(start <= (self.data_table.index +timedelta(milliseconds=self.granularity))) & (end > self.data_table.index)]\n",
    "\n",
    "            # and add 1 to the rows if we take the sum\n",
    "            if aggregation == 'sum':\n",
    "                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] += 1\n",
    "            # or set to 1 if we just want to know it happened\n",
    "            elif aggregation == 'binary':\n",
    "                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] = 1\n",
    "            else:\n",
    "                raise ValueError(\"Unknown aggregation '\" + aggregation + \"'\")\n",
    "\n",
    "    # This function returns the column names that have one of the strings expressed by 'ids' in the column name.\n",
    "    def get_relevant_columns(self, ids):\n",
    "        relevant_dataset_cols = []\n",
    "        cols = list(self.data_table.columns)\n",
    "\n",
    "        for id in ids:\n",
    "            relevant_dataset_cols.extend([col for col in cols if id in col])\n",
    "\n",
    "        return relevant_dataset_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Chapter 2: Initial exploration of the dataset.\n",
    "\n",
    "\"\"\"\n",
    "First, we set some module-level constants to store our data locations. These are saved as a pathlib.Path object, the\n",
    "preferred way to handle OS paths in Python 3 (https://docs.python.org/3/library/pathlib.html). Using the Path's methods,\n",
    "you can execute most path-related operations such as making directories.\n",
    "sys.argv contains a list of keywords entered in the command line, and can be used to specify a file path when running\n",
    "a script from the command line. For example:\n",
    "$ python3 crowdsignals_ch2.py my/proj/data/folder my_dataset.csv\n",
    "If no location is specified, the default locations in the else statement are chosen, which are set to load each script's\n",
    "output into the next by default.\n",
    "\"\"\"\n",
    "\n",
    "DATASET_PATH = Path(sys.argv[1] if len(sys.argv) > 1 else '/Users/robinschijf/Desktop/data/')\n",
    "RESULT_PATH = Path('./intermediate_datafiles/')\n",
    "RESULT_FNAME = sys.argv[2] if len(sys.argv) > 2 else 'chapter2_result.csv'\n",
    "\n",
    "# Set a granularity (the discrete step size of our time series data). We'll use a course-grained granularity of one\n",
    "# instance per minute, and a fine-grained one with four instances per second.\n",
    "GRANULARITIES = [60000, 250]\n",
    "\n",
    "# We can call Path.mkdir(exist_ok=True) to make any required directories if they don't already exist.\n",
    "[path.mkdir(exist_ok=True, parents=True) for path in [DATASET_PATH, RESULT_PATH]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating numerical datasets from files in -f using granularity 60000.\n",
      "Reading data from data_accel_phone.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f4e7fc28b791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_numerical_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_accel_phone.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Timestamps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc_phone_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_numerical_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_accel_watch.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Timestamps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc_watch_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b41911261232>\u001b[0m in \u001b[0;36madd_numerical_dataset\u001b[0;34m(self, file, timestamp_col, value_cols, aggregation, prefix)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_rows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown aggregation {aggregation}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = []\n",
    "for milliseconds_per_instance in GRANULARITIES:\n",
    "    print(f'Creating numerical datasets from files in {DATASET_PATH} using granularity {milliseconds_per_instance}.')\n",
    "\n",
    "    # Create an initial dataset object with the base directory for our data and a granularity\n",
    "    dataset = CreateDataset(DATASET_PATH, milliseconds_per_instance)\n",
    "\n",
    "    \n",
    "    dataset.add_numerical_dataset('data_accel_phone.csv', 'Timestamps', ['x','y','z'], 'avg', 'acc_phone_')\n",
    "    dataset.add_numerical_dataset('data_accel_watch.csv', 'Timestamps', ['x','y','z'], 'avg', 'acc_watch_')\n",
    "\n",
    "    # We add the gyroscope data (continuous numerical measurements) of the phone and the smartwatch\n",
    "    # and aggregate the values per timestep by averaging the values\n",
    "    dataset.add_numerical_dataset('data_gyro_phone.csv', 'Timestamps', ['x','y','z'], 'avg', 'gyr_phone_')\n",
    "    dataset.add_numerical_dataset('data_gyro_watch.csv', 'Timestamps', ['x','y','z'], 'avg', 'gyr_watch_')\n",
    "\n",
    "\n",
    "    # Get the resulting pandas data table\n",
    "    dataset = dataset.data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
